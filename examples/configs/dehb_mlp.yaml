
defaults:
  - _self_
  - override hydra/sweeper: HyperDEHB

learning_rate: constant
learning_rate_init: 0.001
batch_size: 200
n_neurons: 10
n_layer: 1
solver: adam
activation: tanh

seed: 42
epochs: 10  # Default number of epochs

hydra:
  sweeper:
    n_trials: 50
    budget_variable: epochs
    sweeper_kwargs:
      max_parallelization: 0.4
      optimizer_kwargs:
          _target_: dehb.DEHB
          _partial_: true
          n_workers: 1
          min_fidelity: 1
          max_fidelity: 10
    search_space:
      hyperparameters:
        n_layer:
          type: uniform_int
          lower: 1
          upper: 5
          default: ${n_layer}
        n_neurons:
          type: uniform_int
          lower: 8
          upper: 1024
          log: true
          default_value: ${n_neurons}
        activation:
          type: categorical
          choices: [ logistic, tanh, relu ]
          default_value: ${activation}
        solver:
          type: categorical
          choices: [ lbfgs, sgd, adam ]
          default_value: ${solver}
        batch_size:
          type: uniform_int
          lower: 30
          upper: 300
          default_value: ${batch_size}
        learning_rate:
          type: categorical
          choices: [ constant, invscaling, adaptive ]
          default_value: ${learning_rate}
        learning_rate_init:
          type: uniform_float
          lower: 0.0001
          upper: 1
          default_value: ${learning_rate_init}
          log: true
      conditions:
        - child: batch_size
          parent: solver
          type: IN
          values: [ sgd, adam ]
        - child: learning_rate
          parent: solver
          type: EQ
          value: sgd
        - child: learning_rate_init
          parent: solver
          type: IN
          values: [ sgd, adam ]
  run:
    dir: ./tmp/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ./tmp/${now:%Y-%m-%d}/${now:%H-%M-%S}